{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "import dialogflow\n",
    "import time\n",
    "\n",
    "#For Emotion Detection\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "#For speech recognition and audio conversion\n",
    "import speech_recognition as sr\n",
    "from playsound import playsound\n",
    "import win32com.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all the Input Image names for training that is present inside the images directory(File name is used as the name of the person)\n",
    "path = 'C://Users//pc//Desktop//images//'\n",
    "known_face_names = []\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.jpg' in file:            \n",
    "            known_face_names.append(file.split('.')[0])\n",
    "            \n",
    "#Loading each image file and calculating face encoding\n",
    "known_face_encodings = []\n",
    "for i in known_face_names:\n",
    "        image = face_recognition.load_image_file(\"images/\"+i+\".jpg\")\n",
    "        face_encoding = face_recognition.face_encodings(image)[0]\n",
    "        known_face_encodings.append(face_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "\n",
    "def face_recognize(frame):\n",
    "    \"\"\"Function to Recognize all the known persons from the Image\"\"\"\n",
    "    \n",
    "    Name = None\n",
    "        \n",
    "    #resizing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "    \n",
    "    #Finding the face Encodings for the faces in the image\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "    \n",
    "    #Comparing with the trained images and finding a best match\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Friend\"\n",
    "\n",
    "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "        face_names.append(name)\n",
    "        Name=name\n",
    "        \n",
    "    return (face_locations, face_names,Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Neural Networks Model for Emotion Detection\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"unexcited\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for finding the Emotion for the detected faces.\n",
    "def emotion(frame):\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        return emotion_dict[maxindex] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for conversion of text into Audio (using Microsoft Speech API)\n",
    "speaker = win32com.client.Dispatch(\"SAPI.SpVoice\")\n",
    "def audio(mytext):  \n",
    "    speaker.Speak(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for conversion of audio to text (using google speech api)\n",
    "def audtotext():\n",
    "    r = sr.Recognizer()  \n",
    "    with sr.Microphone() as source:\n",
    "        playsound(\"notification.mp3\")\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audi = r.listen(source)        \n",
    "        try:\n",
    "            text = r.recognize_google(audi) \n",
    "        except:\n",
    "            text = \"xxxxx\"\n",
    "            return text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dialogflow Connection Details\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'Baymax-f894a3f33511.json'\n",
    "DIALOGFLOW_PROJECT_ID = 'baymax-dkmasl'\n",
    "DIALOGFLOW_LANGUAGE_CODE = 'en'\n",
    "SESSION_ID = 'me'\n",
    "session_client = dialogflow.SessionsClient()\n",
    "session = session_client.session_path(DIALOGFLOW_PROJECT_ID, SESSION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(name_emot):\n",
    "    det = list(name_emot.items())\n",
    "    if (det[0][1]==None):\n",
    "        return (det[0][0],\"unexcited\")\n",
    "    else:\n",
    "        return (det[0][0],det[0][1])    \n",
    "\n",
    "\n",
    "#Chatbot Function for the chat\n",
    "def speek():\n",
    "    while(True):\n",
    "        text_to_be_analyzed = audtotext()\n",
    "        if text_to_be_analyzed==\"xxxxx\":\n",
    "            audio(\"Sorry could not recognize your voice please give text input\")\n",
    "            text_to_be_analyzed = input()\n",
    "            \n",
    "        else:\n",
    "            t = input(\"you have said \"+text_to_be_analyzed+\" is this correct? (yes/no)\")\n",
    "            if(t.lower()==\"no\" or t.lower()==\"wrong\"or t.lower()==\"incorrect\"):\n",
    "                text_to_be_analyzed=input(\"Enter the correct sentence\")\n",
    "        \n",
    "        text_input = dialogflow.types.TextInput(text=text_to_be_analyzed, language_code=DIALOGFLOW_LANGUAGE_CODE)\n",
    "        query_input = dialogflow.types.QueryInput(text=text_input)\n",
    "        try:\n",
    "            response = session_client.detect_intent(session=session, query_input=query_input)\n",
    "        except InvalidArgument:\n",
    "            raise\n",
    "        \n",
    "        audio(response.query_result.fulfillment_text)\n",
    "        \n",
    "        print(\"Baymax: \",response.query_result.fulfillment_text)\n",
    "        \n",
    "        res = text_to_be_analyzed.lower()\n",
    "    \n",
    "        if(res=='bye' or res=='goodbye' or res=='bye bye'):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hello I can't see you please come closer\n",
      "Baymax:  Hi Aakash Looks Like you are Angry Do you feel disappointed ? Because Anger and diappoinment ups your stroke risk. So try to be happy\n",
      "you have said fever is this correct? (yes/no)yes\n",
      "Baymax:  Stay in bed rest. Remove extra layers of clothing and blankets, unless you have the chills.Take hot water for Bath.You will feel better\n",
      "you have said bhai is this correct? (yes/no)no\n",
      "Enter the correct sentencebye\n",
      "Baymax:  Bye bye  Love you\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    #capturing the Frame from the camera       \n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    #finding all the faces\n",
    "    face_locations, face_names,name = face_recognize(frame.copy())   \n",
    "    no_human = True\n",
    "    \n",
    "    name_emot = {}\n",
    "    \n",
    "    if name!=None:\n",
    "        no_human = False\n",
    "    \n",
    "    if no_human:\n",
    "        audio(\"Hello I can't see you please come closer\")\n",
    "        print(\"Baymax: \",\"Hello I can't see you please come closer\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Detecting the Experession of the Face\n",
    "        \n",
    "        emt = emotion(frame.copy())\n",
    "        name_emot[name] = emt\n",
    "        name,expression = fun(name_emot)\n",
    "        \n",
    "        #get the response for the person from the dialogflow server\n",
    "        \n",
    "        text_input = dialogflow.types.TextInput(text=expression, language_code=DIALOGFLOW_LANGUAGE_CODE)\n",
    "        query_input = dialogflow.types.QueryInput(text=text_input)\n",
    "        \n",
    "        try:\n",
    "            response = session_client.detect_intent(session=session, query_input=query_input)\n",
    "        except:\n",
    "            print(\"Some Error\")\n",
    "            \n",
    "        audio(\"Hi \"+name+\" Looks Like you are \"+expression+\" \"+response.query_result.fulfillment_text)\n",
    "        print(\"Baymax: \",\"Hi \"+name+\" Looks Like you are \"+expression+\" \"+response.query_result.fulfillment_text)\n",
    "        \n",
    "        #After detecting start the chat\n",
    "        speek()\n",
    "        \n",
    "        break\n",
    "            \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reference:</b><br>\n",
    "https://github.com/ageitgey/face_recognition\n",
    "\n",
    "https://github.com/ageitgey/face_recognition/issues/175#issue-257710508\n",
    "\n",
    "<b>Requirements</b><br>\n",
    "https://pypi.org/project/cmake/\n",
    "\n",
    "https://pypi.org/project/dlib/\n",
    "\n",
    "https://pypi.org/project/face-recognition/\n",
    "\n",
    "https://pypi.org/project/SpeechRecognition/\n",
    "\n",
    "https://pypi.org/project/playsound/\n",
    "\n",
    "https://pypi.org/project/pywin32/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
